% % include the figures path relative to the master file
 \graphicspath{ {./content/experiment/figures/} }

\section{Experiments}
\label{sec:exp}

% The aim of this work is to stress some of the design choices done in
% \cite{Lemaintre2015miccaiOCT}, further test the influence of the new blocks
% composing our framework and compare in comparison to its earlier version.

This section describes the conceived experimentation to investigate the effects
of (i) optimal number of words, (ii) different pre-processing steps, and (iii)
different classifiers.
Table~\ref{tab:experiment_summary} reports the experimentation
in~\cite{Lemaintre2015miccaiOCT} as a baseline, and outlines the complementary
experimentation here proposed.
The reminder of this section details experiment commonalities, while experiment particularities are reported in the following subsections, and a complete set of the results obtained at each experiment is found in Appendix~\ref{app:1}.

All the experiments are performed using our own dataset reported in Sect.\,\ref{sec:exp:dataset:seri} and reported according to Sect.\,\ref{sec:exp:eval}.
\ac{lbp} and \ac{lbptop} features are extracted for different sampling points of 8, 16, and 24 for radius of 1, 2, and 3, respectively.
Both \emph{local}- and \emph{global}-mapping strategies are used when applicable.
The partitioning for \emph{local}-mapping is set to ($7 \times 7$) patch for 2D \ac{lbp} and ($ 7 \times 7 \times 7$) sub-volume for \ac{lbptop}.

\input{content/experiment/experiment-table.tex}

\subsection{Dataset}\label{sec:exp:dataset:seri}
This data was acquired by the Singapore Eye Research Institute (SERI), using CIRRUS TM (Carl Zeiss Meditec, Inc., Dublin, CA) \ac{sdoct} device. The datasets consist of 32 \ac{oct} volumes (16 \ac{dme} and 16 normal cases). Each volume contains 128 B-scan with resolution of 512 $\times$ 1024 pixels.
All \ac{sdoct} images are read and assessed by trained graders and identified as normal or \ac{dme} cases based on evaluation of retinal thickening, hard exudates, intraretinal cystoid space formation and subretinal fluid.

\subsection{Evaluation}\label{sec:exp:eval}
Accordingly to~\cite{Lemaintre2015miccaiOCT}, all the experiments are evaluated in terms of \acf{se} and \acf{sp} using \ac{lopocv} strategy.

\ac{se} and \ac{sp} are statistics driven from the confusion matrix (see Fig.\,\ref{fig:CM}) as stated in Eq.\,\ref{eq:sesp}.
The \ac{se} evaluates the performance of the classifier with respect to the positive class, while the \ac{sp} evaluates its performance with respect to negative class.

\begin{figure}
\begin{center}
\begin{tikzpicture}[scale=0.4]
      \node at (1,1){
      \scriptsize{
        \begin{tabular}{
            >{\centering}m{1em} >{\centering}m{1em} >{\centering}m{1in} >{\centering\arraybackslash}m{1in}}
          % c>{\centering}m{2em}ccc}
          & & \multicolumn{2}{c}{ Actual}\\
          & & A+ & A- \\
          \cline{3-4}
          & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{}\\
          \multirow{3}{*}{\rotatebox[origin=c]{90}{Predicted}}& \multicolumn{1}{c|}{P+} &  \multicolumn{1}{c|}{True Positive (TP)} & \multicolumn{1}{c|}{False Positive (FP)} \\
          &\multicolumn{1}{c|}{}  & \multicolumn{1}{c|}{}& \multicolumn{1}{c|}{} \\
          \cline{3-4}
          & \multicolumn{1}{c|}{} &\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{}\\

          & \multicolumn{1}{c|}{P-} &\multicolumn{1}{c|}{False Negative (FN)}  &\multicolumn{1}{c|}{True Negative (TN)}\\
          & \multicolumn{1}{c|}{} &\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{}\\
          \cline{3-4}
          \end{tabular}
      }};
    \end{tikzpicture}
    \end{center}
\caption{Confusion matrix with true and false positive detected samples (\acs{tp}, \acs{fp}) in the first row, from left to right and the false and true negative detected samples (\acs{fn}, \acs{tn}) in the second row, from left to right.}
\label{fig:CM}
\end{figure}

\begin{align}
 \ac{se}  = \frac{TP}{TP+FN} \qquad \ac{sp} = \frac{TN}{TN+FP}
 \label{eq:sesp}
\end{align}

The usage of \ac{lopocv} implies that at each round, a pair \ac{dme}-normal volume is selected for testing while the remaining volumes are used for training.
Subsequently, no \ac{se} or \ac{sp} variance can be reported.
However, \ac{lopocv} strategy has been adopted despite this limitation due to the reduced size of the dataset.

\subsection{Experiment \#1: \acs{bow} dictionary size}\label{subsec:exp1}
% Experiment structure
%
% Intro:
%   - background
%   - goal / experiment intention / why
%   - data
%   - evaluation
%   - reference to result table
%
% Procedure (by data if more than one):
%   - pre-processing
%   - feature extraction
%   - mapping
%   - feature representation
%   - classifier
%
% Remarks (if any)
%
% Result highlights:
%   - (only a description)

%% Experiment intro
On its original implementation~\cite{Lemaitre2015, Lemaintre2015miccaiOCT} the codebook size was arbitrarily set $k=32$.
This experiment evaluates several codebook sizes to find optimal size of the codebook in variated conditions.

%% Experiment procedure
Several pre-processing strategies are evaluated: (i) \nlm, (ii) a combination of \nlm and flattening, and (iii) a combination of \nlm, flattening and aligning.
\lbp and \lbptop descriptors are detected using the default configuration.
Volumes are represented using \ac{bow}, where the codebook size ranging for $k \in \{10, 20, 30, \cdots, 100, 200, \cdots, 500, 1000\}$.
Finally, the volumes are classified using \lr.
The choice of this linear classifier avoids that the results get boosted by the classifier. In this manner any improvement would be linked to the pre-processing and the size of the codebook.
%\input{content/experiment/Table2.tex}

The usual build of the codebook consists of clustering the samples in the feature space using $k$-means (see Sect.\,\ref{subsec:fearep}).
However, this operation is rather computationally expensive and convergence of the $k$-means algorithm for all codebook sizes is not granted.
Nonetheless, Nowak\,\textit{et al.}~\cite{nowak2006sampling} pointed out that randomly generated codebooks can be used at the expenses of accuracy.
Thus, the codebook are randomly generated since the final aim is to asses the influence of codebook size and not the performance of the framework.
For this experiment, the codebook building is carried out using random initialization $k$-means++ algorithm~\cite{arthur2007k}, which is usually used as a $k$-means initialization algorithm.

For this experiment, \ac{se} and \ac{sp} are complemented with \ac{acc} and \ac{f1} as stated in Eq.\,\ref{eq:accf1}.
\ac{acc} offers an overall sense of the classifier performance while \ac{f1} illustrates the trade off between \ac{se} and precision.

Appendix~\ref{app:1}\,-\,Table~\ref{tab:table2} shows the results obtained for
the optimal dictionary size, while the complete set of \ac{acc} and \ac{f1}
score graphs\footnote{\texttt{http://tinyurl.com/jeczfh6}} can be reproduced
at~\cite{Lemaitre2015}.  In order to illustrate the impact of the dictionary
size, Fig.\,\ref{fig:RBOW} illustrates the \ac{acc} and \ac{f1} score graphs
for \added[id=sik]{xxxxxx xxxx xxx xx} particular case.

\begin{align}
\ac{acc} = \frac{TP+TN}{TP+TN+FP+FN} \qquad \ac{f1} = \frac{2TP}{2TP +FP+FN}
\label{eq:accf1}
\end{align}

\begin{figure}
\centering
\includegraphics[width=0.85\textwidth]{figure2}
\caption{The performance of \ac{lr} with \ac{nlm}+\ac{f} pre-processing for different $P$ and $R$.}
\label{fig:RBOW}
\end{figure}

The results show that in general, the influence of the pre-processing is not
substantial nor consistent and that larger \ac{lbp} radius tend to produce
better results when small dictionaries are used.

\subsection{Experiment \#2}\label{subsec:exp2}
% Experiment structure
%
% Intro:
%   - background
%   - goal / experiment intention / why
%   - data
%   - evaluation
%   - reference to result table
%
% Procedure (by data if more than one):
%   - pre-processing
%   - feature extraction
%   - mapping
%   - feature representation
%   - classifier
%
% Remarks (if any)
%
% Result highlights:
%   - (only a description)

%% Experiment intro
This experiment explores the improvement associated with: (i) different pre-processings, and (ii) using larger range of classifiers (i.e. linear and non-linear) for high represented features.

%% Procedure
Similar pre-processing strategies to the previous experiment are evaluated (NLM, NLM+\acs{f}, and NLM+\acs{fal}).
In this experiment the codebooks for the \ac{bow} representation of \ac{lbp} and \ac{lbptop} features are computed using regular $k$-means algorithm which is initialized using $k$-means++, where $k$ is chosen according to the findings of \emph{Experiment \#1}.
Finally, the volumes are classified using $k$-\ac{nn}, \ac{rf}, \ac{gb}, and \ac{svm}.
%For this experiment, several pre-processing strategies are evaluated: (i) \nlm, (ii) a combination of \nlm and flattening, and (iii) a combination of \nlm, flattening and aligning.
%\lbp and \lbptop features are detected using the default configuration and volumes are represented using \bow.
%The codebooks are computed using regular $k$-means algorithm which is initialized using $k$-means++, where $k$ is chosen according to the findings of \emph{Experiment \#1}.
%Finally, the volumes are classified using $k$-\ac{nn}, \ac{rf}, \ac{gb}, and \ac{svm}.
Regarding the classification strategies, $k$-\ac{nn} classifier is trained by considering the 3 nearest neighbor.
The \ac{rf} and \ac{gb} classifier are trained using 100 un-pruned trees, while \ac{svm} classifier is trained with \ac{rbf} kernel and its parameters $C$, and $\gamma$ are optimized through grid-search.

Complete list of the obtained results from this experiment are shown in Appendix~\ref{app:1} - Table~\ref{tab:table3},\deleted[id=sik, remark={state this at the table caption}]{while the most relevant configurations are shaded and the highest results are highlighted in \textbf{bold}}. Despite that highest performance is achieved when flattening or flattening and aligning, most configurations decline when applying these pre-processing stages. Best results are achieved using \ac{svm} followed by \ac{rf}.

%%% Experiment Result description
%%Table~\ref{tab:table3} shows the obtained results from this experiment, the most relevant configurations are shaded and the highest results are highlighted in \textbf{bold}.
%Regarding the effects of pre-processing, most configurations decrease in performance while aligning and flattening the B-scan (i.e. light shaded configurations in Table~\ref{tab:table3}).
%%the performance of the most configurations decreases by aligning or flattening the B-scan (i.e. light shaded configurations in Table~\ref{tab:table3}).
%%$k$-NM\,8\,\emph{local}-\lbp, \svm\,8\,\emph{local}-\lbptop, \svm\,16\,\emph{local}-\lbptop, \rf\,8\,\emph{global}-\lbp, \rf\,8\,\emph{local}-\lbp).
%However, the two best configurations (i.e. dark shaded in Table~\ref{tab:table3}), achieve better results when adding flattening or flattening and alignment as pre-processing.
%A small radius and small number of samples in feature detection tends to increase the classification performance.
%%(see: \svm\,8\,\emph{local}-\lbp and \rf\,16\,\emph{local}-\lbptop).
%%Feature detection shows a tendency towards better results when using small radius and small number of samples.
%Regarding the mapping strategy, local mapping tends to produce better results than global mapping.
%In terms of choosing a classifier, \ac{svm} provides the best results, followed by \rf.
%
%The best results (81.2\%\,\ac{se} and 93.7\%\,\ac{sp}) are achieved using \nlm, flattening, \lbp detection using $\{P,R\} = \{8,1\}$, local mapping, high-level representation, using a codebook with $k=70$, and \svm classifier.
%This result can be compared with other relevant results in Table~\ref{tab:results_summary}.

\subsection{Experiment \#3}\label{subsec:exp3}
% Experiment structure
%
% Intro:
%   - background
%   - goal / experiment intention / why
%   - data
%   - evaluation
%   - reference to result table
%
% Procedure (by data if more than one):
%   - pre-processing
%   - feature extraction
%   - mapping
%   - feature representation
%   - classifier
%
% Remarks (if any)
%
% Result highlights:
%   - (only a description)

%% Experiment intro
This experiment replicates the \emph{Experiment \#2} for the case of low-level represented features from the volumes.

%% Experiment procedure
The same pre-processing strategies (NLM, NLM+\acs{f}, and NLM+\acs{fal}) are investigated.
%While investigating the same pre-processing strategies (\ac{nlm}, \ac{nlm}+\acs{f}, and \ac{nlm}+\acs{fal}), the \ac{lbp} and \ac{lbptop} features are extracted using global mapping and represented in low level.
%For this experiment, several pre-processing strategies are evaluated: (i) \nlm, (ii) a combination of \nlm and flattening, and (iii) a combination of \nlm, flattening and aligning.
\lbp and \lbptop descriptors are detected using the default configuration.
Volumes are represented using low-level feature representation of the \emph{global} mapping.
Finally, the volumes are classified using $k$-\ac{nn}, \ac{rf}, \ac{gb}, and \ac{svm}.
The obtained results from this experiment are listed in Appendix~\ref{app:1} - Table~\ref{tab:table4}.
Flattening the B-scan boosts the results for the best performing configuration, but this effect is not consistent across all the configurations.
In terms of classifier, \rf have better performance than the others but the highest \ac{sp} is achieved using \svm.

%%% Experiment Result description
%The obtained results from this experiment is listed in Table~\ref{tab:table4}.
%The most relevant configurations are shaded and the highest results are highlighted in \textbf{bold}.
%Similarly to the results reported in Sect.\,\ref{subsec:exp3}, the effect of flattening the B-scan boosts the results for the best performing configuration, but this effect is not consistent across all the configurations.
%For this experiment, \lbptop outperforms \lbp and larger $P$ and $R$ values for feature detection tends to obtain better results.
%In terms of classifier, \rf have better performance than the others but the highest \ac{sp} is achieved using \svm.

% The highest results of this experiment, \ac{se} and \ac{sp} of 81.2\% and 81.2\%, respectively, was achieved with \ac{rf} and using \emph{global}-\ac{lbptop} features with sampling points and radius of $\{S,R\}=\{24,3\}$.
% In general, in this experiment, \emph{global}-\ac{lbptop} features have better performance in comparison to \emph{global}-\ac{lbp} features and the  classification rates improved while using a higher number of sampling points and radius ($\{S,R\}=\{24,3\}$).

% Similar to the previous experiments, although the effects of additional pre-processing steps (\ac{f} and \ac{fal}) is evident for \ac{rf} performance on $\{S,R\} = \{24,3\}$, similar to the previous experiments, this influence is not consistent for all different configurations, in terms of classifier and $\{S,R\}$.

%The best results (81.2\%\,\ac{se} and 81.2\%\,\ac{sp}) are achieved when using \nlm, flattening, \lbptop detection using $\{P,R\}= \{24,3\}$, global mapping, low-level representation, and \rf classifier.
%This result can be compared with other relevant results in Table~\ref{tab:results_summary}
%\input{content/experiment/Table3.tex}
%\input{content/experiment/Table4.tex}

\section{Results}\label{sec:res}
Table~\ref{tab:results_summary} represents a summary and comparison of the obtained results from experiment~\#2~\&~\#3 with respect to the baseline.
The complete list of obtained result regarding each experiment can be found in Appendix~\ref{app:1} (see Table~\ref{tab:table2}, \ref{tab:table3}, and \ref{tab:table4}).
This table illustrates the highest 26 performances from highest to lowest considering their achieved \ac{se} and \ac{sp}.
The related experiment, choice of pre-processing, type and configuration of the features, choice of mapping and representation, choice of classifier, and finally if required size of codebook is illustrated in this table.
\input{content/experiment/Table5.tex}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../main.tex"
%%% End:

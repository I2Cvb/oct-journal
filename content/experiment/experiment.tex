% % include the figures path relative to the master file
% \graphicspath{ {./content/results/figures/} }

\section{Experiments and Validation}
\label{sec:exp} \label{sec:exp:datasets}
\todo[inline]{add repository reference somewhere}

To evaluate the effects and influence of the different blocks composing our framework, an experimentation suit has been designed to test different configuration parameters, which are evaluated using different datasets (see Table.~\ref{tab:experiment_summary}).
The rest of this section details aspects of the experimentation and the design decisions that are consistent across all the experimentation, while subsections report different technicalities.

Unless stated otherwise, all the experiments are run using our own dataset (SERI) alone.
Only for the sake of comparison some experiments are re-run on the Duke public dataset using our optimal configurations.
SERI and Duke dataset details are reported in \cref{sec:exp:dataset:seri} and \cref{sec:exp:dataset:duke} respectively.

For all the experiments, \ac{lbp} and \ac{lbptop} features are extracted for different sampling points of 8, 16, and 24 for radius of 1, 2, and 3, respectively.
As previously mentioned, two different mapping strategies, \emph{local} and \emph{global}, are used, where for \emph{local} mapping, we consider a ($7 \times 7$) \acf{sw} for 2D \ac{lbp} and ($ 7 \times 7 \times 7$) sub-volume for \ac{lbptop}.

All the experiments are evaluated using \ac{lopocv} strategy.
In this validation, at each round a pair \ac{dme}-normal volume is selected for testing while the rest of the volumes are used for training.
The use of this method implies that no variance in terms of \ac{se} and \ac{sp} can be reported.
However, and despite this limitation, \ac{lopocv} has been employed due to the small size of the dataset.

The obtained results of all the experiments except experiment \#2 is represented in terms of \ac{se} and \ac{sp}, which are statistics driven from the confusion matrix (see Fig.~\ref{fig:CM}) as stated in \cref{eq:sesp}.
The \ac{se} evaluates the performance of the classifier with respect to the positive class, while the \ac{sp} evaluate it's performance with respect to negative class.
\begin{align}
 \ac{se}  = \frac{TP}{TP+FN} \qquad \ac{sp} = \frac{TN}{TN+FP}
 \label{eq:sesp}
\end{align}

Some experimentation is complemented using \ac{acc} and \ac{f1}.
\acl{acc} is used to have a overall sense of classifier performance, and \ac{f1} is used to see the trade off between \ac{se} and precision.
Equation.~\ref{eq:accf1} shows the formulation of these two measurements.
\begin{align}
\ac{acc} = \frac{TP+TN}{TP+TN+FP+FN} \qquad \ac{f1} = \frac{2TP}{2TP +FP+FN}
\label{eq:accf1}
\end{align}

\begin{figure}
\begin{center}
\begin{tikzpicture}[scale=0.4]
      \node at (1,1){
      \scriptsize{
        \begin{tabular}{
            >{\centering}m{1em} >{\centering}m{1em} >{\centering}m{1in} >{\centering\arraybackslash}m{1in}}
          % c>{\centering}m{2em}ccc}
          & & \multicolumn{2}{c}{ Actual}\\
          & & A+ & A- \\
          \cline{3-4}
          & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{}\\
          \multirow{3}{*}{\rotatebox[origin=c]{90}{Predicted}}& \multicolumn{1}{c|}{P+} &  \multicolumn{1}{c|}{True Positive (TP)} & \multicolumn{1}{c|}{False Positive (FP)} \\
          &\multicolumn{1}{c|}{}  & \multicolumn{1}{c|}{}& \multicolumn{1}{c|}{} \\
          \cline{3-4}
          & \multicolumn{1}{c|}{} &\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{}\\

          & \multicolumn{1}{c|}{P-} &\multicolumn{1}{c|}{False Negative (FN)}  &\multicolumn{1}{c|}{True Negative (TN)}\\
          & \multicolumn{1}{c|}{} &\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{}\\
          \cline{3-4}
          \end{tabular}
      }};
    \end{tikzpicture}
    \end{center}
\caption{Confusion matrix with truly and falsely positive detected samples (\acs{tp}, \acs{fp}) in the first row, from left to right and the falsely and truly negative detected samples (\acs{fn}, \acs{tn}) in the second row, from left to right.}
\label{fig:CM}
\end{figure}


Experimentation details can be found in Sect.\,\ref{subsec:exp1} to Sect.\,\ref{subsec:exp4} and summarized in Table.~\ref{tab:experiment_summary}.
In general terms, all the experiments have been carried out using SERI dataset while \emph{Experiment \#1~(Sect.\,\ref{subsec:exp1})} has been complemented using Duke dataset for comparison purposes. This \emph{Experiment \#1\,\cref{subsec:exp1}} takes from the experimentation reported in \cite{Lemaintre2015miccaiOCT} to evaluate the effects of different feature representations\todo{are not we evaluating feature extraction?} and compares the results to those obtained by Venhuizen\emph{et al.}\,\cite{Venhuizen2015}.
\emph{Experiment \#2~(Sect.\,\ref{subsec:exp2})} studies the effect of the codebook size in order to find the optimal number of words for our application when using \ac{bow}.
\emph{Experiment \#3~(Sect.\,\ref{subsec:exp3})} studies the effect different pre-processing and classifiers.

\input{content/experiment/experiment-table.tex}




\subsection{SERI-Dataset}\label{sec:exp:dataset:seri}
This data was acquired by Singapore Eye Research Institute (SERI), using CIRRUS TM (Carl Zeiss Meditec, Inc., Dublin, CA) \ac{sdoct} device. The datasets consist of 32 \ac{oct} volumes (16 \ac{dme} and 16 normal cases). Each volume contains 128 B-sane with  dimension of 512 $\times$ 1024 pixels.  All \ac{sdoct} images are read and assessed by trained graders and identifies as normal or \ac{dme} cases based on evaluation of retinal thickening, hard exudates, intraretinal cystoid space formation and subretinal fluid.

\subsection{Duke-Dataset} \label{sec:exp:dataset:duke}
This data published by Srinivasan\,\emph{et al.}~\cite{Srinivasan2014} was acquired in Institutional Review Board-approved protocols using Spectralis \ac{sdoct} (Heidelberg Engineering Inc., Heidelberg, Germany) imaging at Duke University, Harvard University and the University of Michigan. This datasets consist of 45 \ac{oct} volumes (15 \ac{amd}, 15 \ac{dme} and 15 normal). In this study we only consider a subset of the original data containing 15 \ac{dme} and 15 normal \ac{oct} volumes.


\subsection{Experiment \#1}\label{subsec:exp1}

% Experiment structure
%
% Intro:
%   - background
%   - goal / experiment intention / why
%   - data
%   - evaluation
%   - reference to result table
%
% Procedure (by data if more than one):
%   - pre-processing
%   - feature extraction
%   - mapping
%   - feature representation
%   - classifier
%
% Result highlights:
%   - (only a description)

%% Experiment intro
For the completeness of this article, this experiment replicates some of the experiments reported in~\cite{Lemaintre2015miccaiOCT}.
The experiment evaluates the effects of different \added[id=sik]{feature extraction, feature representation,\ldots} using the SERI and Duke datasets.

%% Experiment Procedure
For this experiment, the volumes are pre-processed using \nlm.
\lbp and \lbptop descriptors are extracted using the default configuration.
Local and global mapping are used.
Volumes are represented using both low-level and high-level representation. For concordance with~\cite{Lemaintre2015miccaiOCT}, when using \bow the size of the coodebook is fixed to $32$ words.
Finally, the volumes are classified using \ac{rf} classifier with 100 un-pruned trees.

Results are listed in Table.~\ref{tab:table1-1}, while the two best performing configurations are compared to Venhuizen\,\textit{et al.}~\cite{Venhuizen2015} in Table.~\ref{tab:table1-2}.

%% Experiment Result description
\added[id=sik]{Result description}

\input{content/experiment/Table1.tex}
\input{content/experiment/Table1-2.tex}

\subsection{Experiment \#2}\label{subsec:exp2}
% Experiment structure
%
% Intro:
%   - background
%   - goal / experiment intention / why
%   - data
%   - evaluation
%   - reference to result table
%
% Procedure (by data if more than one):
%   - pre-processing
%   - feature extraction
%   - mapping
%   - feature representation
%   - classifier
%
% Remarks (if any)
%
% Result highlights:
%   - (only a description)

%% Experiment intro
In order to determine the optimal size of the codebook when using \bow, this experiment evaluates several codebook sizes on SERI dataset.

%% Experiment procedure
For this experiment, several pre-processing strategies are evaluated: (i) \nlm, (ii) a combination of \nlm and flattening; (iii) a combination of \nlm, flattening and aligning.
\lbp and \lbptop descriptors are extracted using the default configuration.
Volumes are represented using the high-level representation \bow using $k \in \{10, 20, 30, \cdots, 100, 200, \cdots, 500, 1000\}$.
Finally, the volumes are classified using \lr. The choice of a linear classifier avoids that the results get busted by the classifier. In this manner any improvement would be linked to the pre-processing and the size of the codebook.


The usual construction of the codebook consists of clustering the samples in
the feature space using $k$-means. However, this operation is rather
computationally expensive and convergence of the $k$-means algorithm for all
codebook sizes is not granted.
Nonetheless, \citeauthor{nowak2006sampling} pointed out that randomly generated coodebooks can be used at expenses of accuracy.
Since the goal is to assess the best coodebook size not its final performance, for this experiment, the construction of the coodebook has been carried out using $k$-means++ algorithm \added[id=sik]{(reference is missing)}, which is usually used as a $k$-means initialization algorithm.

Figure.~\ref{fig:RBOW} shows the \ac{acc} and \ac{f1} score graphs obtained for some of the configurations\footnote{Full set of scores can be found at the github repository in~\cite{Lemaitre2015}}.
The results obtained when using the optimum number of words for each configuration, are listed in Table~\ref{tab:table2}.

\begin{figure}[t]
  \missingfigure{fig:RBOW}
  \caption{}
  \label{fig:RBOW}
\end{figure}


\input{content/experiment/Table2.tex}



\subsection{Experiment \#3}\label{subsec:exp3}
This experiment is performed for high-level features on SERI dataset.
Using the optimum number of words which are obtained from the previous experiment, the low-level feature sets with regards to different pre-processing configurations are re-represented using \ac{bow} and $k$-means clustering approach and are classified using different classifiers such as $k$-\ac{nn}, \ac{rf}, \ac{gb}, and \ac{svm}.\
The $k$-means algorithm is initialized using $k$-means++ method and is performed with 5 iteration for each codebook.
The \ac{rf} and \ac{gb} classifier are trained using 100 un-pruned trees, while \ac{svm} classifier is trained with \ac{rbf} kernel.
The regularization and soft-margin parameters of this classifier are chosen with grid-search method.
Finally the $k$-\ac{nn} classifier is trained by considering the 3 nearest neighbor.
Table~\ref{tab:table3} shows the obtained results from this experiment.\\

\input{content/experiment/Table3.tex}


\subsection{Experiment \#4}\label{subsec:exp4}
This experiment is conducted for low-level features.
The \emph{global} \ac{lbp} and \ac{lbptop} features are classified using the same classifiers as previous experiments with the same configurations.
The obtained results from this experiment is listed in Table.~\ref{tab:table4}.\\
\input{content/experiment/Table4.tex}



% The SERI datasets are provided in complete \ac{oct} volumes by 512$\times$1024$\times$128 dimensions. Using this datasets, first the three low-level features such as \ac{lbp}, \ac{lbp}+\ac{pca} and \ac{lbptop} are extracted. The rotation invariant uniform ($riu2$) descriptors are calculated with the $P$ number of 8, 16 and 24 for the radius if 1, 2 and 3 respectively. The features are classified using RF with 100 tress. Table \ref{tab:LbPTopVolumeResult} shows the relative results for $8riu2$, $16riu2$, $24riu2$ and their combination $8riu2 + 16riu2 + 24riu2$. The results are presented in terms of \ac{se} and \ac{sp} percentages.

% The second experiment is carried out using high-level features and \ac{bow} approach, on SERI datasets. The first high-level feature \ac{lbp}+\ac{bow} is obtained by applying \ac{bow} with 32 visual-words on the previously low-level \ac{lbp} features (applied on each B-scan). The second and third high-level descriptors are obtained using a dense approach by applying the \ac{sw} of size (7$\times$7) on each B-scan and \ac{sw} of size (7$\times$7$\times$7) to the whole volume respectively. \ac{lbp}+\ac{bow}+\ac{sw} represent the second high-level feature where the 2D-\ac{lbp} features are extracted for each sliding window on each B-scan and the visual-words are selected from the pool, consisting of their histograms. The third high-level feature, \ac{lbptop}+\ac{bow}+\ac{sw}, is defined using \ac{lbptop}. By using the sliding window the 3D-\ac{lbp} features are extracted for each patch. Same as previous experiment with low-level features, the descriptors are calculated with the $P$ number of 8, 16 and 24 for the radius if 1, 2 and 3 respectively. The obtained results of this experiment are illustrated in Tab. \ref{tab:SERIBoWResult}.

% In order to compare our proposed framework the third experiment is carried out using the subsection of Duke datasets \cite{Srinivasan2014}. The OCT volumes provided by this datasets are of different volume size, cropped and denoised by the method of authors choice. Subsequently only the second experiment with high-level features and low-level \ac{lbptop} features comply with these requirements. The number of visual-words and the size of \ac{sw} for 2D and 3D features are the same than the previous experiment. The 2D and 3D \ac{lbp} features are extracted with $P$ number of 8, 16 and 24 for the radius if 1, 2 and 3 respectively. The obtained results for this experiment are shown in Tab. \ref{tab:DukeBoWResult}.
%----------

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../main.tex"
%%% End:

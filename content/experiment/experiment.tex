% % include the figures path relative to the master file
% \graphicspath{ {./content/results/figures/} }

\section{Experiments and Validation}
\label{sec:exp} \label{sec:exp:datasets}
\todo[inline]{add repository reference somewhere}

To evaluate the effects and influence of the different blocks composing our framework, an experimentation suit has been designed to test different configuration parameters, which are evaluated using different datasets (see Table.~\ref{tab:experiment_summary}).
The rest of this section details aspects of the experimentation and the design decisions that are consistent across all the experimentation, while subsections report different technicalities.

Unless stated otherwise, all the experiments are run using our own dataset alone, SERI.
Only for the sake of comparison some experiments are re-run on the Duke public dataset using our optimal configurations.
SERI and Duke dataset details are reported in \ref{sec:exp:dataset:seri} and \ref{sec:exp:dataset:duke} respectively.

For all the experiments, \ac{lbp} and \ac{lbptop} features are extracted for different sampling points of 8, 16, and 24 for radius of 1, 2, and 3, respectively.
As previously mentioned, two different mapping strategies, \emph{local} and \emph{global}, are used, where for \emph{local} mapping, we consider a ($7 \times 7$) \acf{sw} for 2D \ac{lbp} and ($ 7 \times 7 \times 7$) sub-volume for \ac{lbptop}.

All the experiments are evaluated using \ac{lopocv} strategy.
In this validation, at each round a pair \ac{dme}-normal volume is selected for testing while the rest of the volumes are used for training.
The use of this method implies that no variance in terms of \ac{se} and \ac{sp} can be reported.
However, and despite this limitation, \ac{lopocv} has been employed due to the small size of the dataset.

All the experiments are evaluated in terms of \ac{se} and \ac{sp}, which are statistics driven from the confusion matrix (see Fig.~\ref{fig:CM}) as stated in \cref{eq:sesp}.
The \ac{se} evaluates the performance of the classifier with respect to the positive class, while the \ac{sp} evaluate it's performance with respect to negative class.
\begin{align}
 \ac{se}  = \frac{TP}{TP+FN} \qquad \ac{sp} = \frac{TN}{TN+FP}
 \label{eq:sesp}
\end{align}

Some experimentation is complemented using \ac{acc} and \ac{f1}.
\acl{acc} is used to have a overall sense of classifier performance, and \ac{f1} is used to see the trade off between \ac{se} and precision.
Equation.~\ref{eq:accf1} shows the formulation of these two measurements.
\begin{align}
\ac{acc} = \frac{TP+TN}{TP+TN+FP+FN} \qquad \ac{f1} = \frac{2TP}{2TP +FP+FN}
\label{eq:accf1}
\end{align}

\begin{figure}
\begin{center}
\begin{tikzpicture}[scale=0.4]
      \node at (1,1){
      \scriptsize{
        \begin{tabular}{
            >{\centering}m{1em} >{\centering}m{1em} >{\centering}m{1in} >{\centering\arraybackslash}m{1in}}
          % c>{\centering}m{2em}ccc}
          & & \multicolumn{2}{c}{ Actual}\\
          & & A+ & A- \\
          \cline{3-4}
          & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{}\\
          \multirow{3}{*}{\rotatebox[origin=c]{90}{Predicted}}& \multicolumn{1}{c|}{P+} &  \multicolumn{1}{c|}{True Positive (TP)} & \multicolumn{1}{c|}{False Positive (FP)} \\
          &\multicolumn{1}{c|}{}  & \multicolumn{1}{c|}{}& \multicolumn{1}{c|}{} \\
          \cline{3-4}
          & \multicolumn{1}{c|}{} &\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{}\\

          & \multicolumn{1}{c|}{P-} &\multicolumn{1}{c|}{False Negative (FN)}  &\multicolumn{1}{c|}{True Negative (TN)}\\
          & \multicolumn{1}{c|}{} &\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{}\\
          \cline{3-4}
          \end{tabular}
      }};
    \end{tikzpicture}
    \end{center}
\caption{Confusion matrix with truly and falsely positive detected samples (\acs{tp}, \acs{fp}) in the first row, from left to right and the falsely and truly negative detected samples (\acs{fn}, \acs{tn}) in the second row, from left to right.}
\label{fig:CM}
\end{figure}


Experimentation details can be found in Sect.\,\ref{subsec:exp1} to Sect.\,\ref{subsec:exp4} and summarized in Table~\ref{tab:experiment_summary}.
In general terms, all the experiments have been carried out using SERI dataset while \emph{Experiment \#1}(Sect.\,\ref{subsec:exp1}) has been complemented using Duke dataset for comparison purposes. This \emph{Experiment \#1}(Sect.\,\ref{subsec:exp1}) takes from the experimentation reported in \cite{Lemaintre2015miccaiOCT} to evaluate the effects of different feature representations and compares the results to those obtained by Venhuizen\emph{et al.}\,\cite{Venhuizen2015}.
\emph{Experiment \#2 and \#3} (Sect.\,\ref{subsec:exp2}\,\&\,\ref{subsec:exp3})) studies the high-level feature extraction from the volumes using \bow. The former experiments effect of the codebook size in order to find the optimal number of words using a linear classifier; while the later explore more sophisticated classifiers using \added(id = moj){based on} the previously found codebook sizes.
\emph{Experiment \#4} (Sect.\,\ref{subsec:exp4}) accounts for the experimentation using low-level representation.

\input{content/experiment/experiment-table.tex}

\subsection{SERI-Dataset}\label{sec:exp:dataset:seri}
This data was acquired by Singapore Eye Research Institute (SERI), using CIRRUS TM (Carl Zeiss Meditec, Inc., Dublin, CA) \ac{sdoct} device. The datasets consist of 32 \ac{oct} volumes (16 \ac{dme} and 16 normal cases). Each volume contains 128 B-sane with  dimension of 512 $\times$ 1024 pixels.  All \ac{sdoct} images are read and assessed by trained graders and identifies as normal or \ac{dme} cases based on evaluation of retinal thickening, hard exudates, intraretinal cystoid space formation and subretinal fluid.

\subsection{Duke-Dataset} \label{sec:exp:dataset:duke}
This data published by Srinivasan\,\emph{et al.}~\cite{Srinivasan2014} was acquired in Institutional Review Board-approved protocols using Spectralis \ac{sdoct} (Heidelberg Engineering Inc., Heidelberg, Germany) imaging at Duke University, Harvard University and the University of Michigan. This datasets consist of 45 \ac{oct} volumes (15 \ac{amd}, 15 \ac{dme} and 15 normal). In this study we only consider a subset of the original data containing 15 \ac{dme} and 15 normal \ac{oct} volumes.


\subsection{Experiment \#1}\label{subsec:exp1}

% Experiment structure
%
% Intro:
%   - background
%   - goal / experiment intention / why
%   - data
%   - evaluation
%   - reference to result table
%
% Procedure (by data if more than one):
%   - pre-processing
%   - feature extraction
%   - mapping
%   - feature representation
%   - classifier
%
% Result highlights:
%   - (only a description)

%% Experiment intro
For the completeness of this article, this experiment replicates some of the experiments reported in~\cite{Lemaintre2015miccaiOCT}, using the SERI and Duke datasets.

%% Experiment Procedure
For this experiment, the volumes are pre-processed using \nlm.
\lbp and \lbptop descriptors are detected using the default configuration.
Local and global mapping are used.
Volumes are represented using both low-level and high-level feature extraction. For concordance with~\cite{Lemaintre2015miccaiOCT}, when using \bow the size of the coodebook is fixed to $32$ words.
Finally, the volumes are classified using \ac{rf} classifier with 100 un-pruned trees.

Results are listed in Table~\ref{tab:table1-1}.
The two configurations achieving the best results in Table~\ref{tab:table1-1} are compared to Venhuizen\,\textit{et al.}~\cite{Venhuizen2015} in Table~\ref{tab:table1-2}. 
%% Experiment Result description
Overall, the obtained results indicate that features driven from \ac{lbp} descriptors are highly discriminative.
Nevertheless, Table~\ref{tab:table1-2} indicates a substantial performance difference between SERI and Duke dataset.
This is attributed to the fact that the volumes in Duke dataset are provided with embedded pre-processing steps.



\input{content/experiment/Table1.tex}
\input{content/experiment/Table1-2.tex}

\subsection{Experiment \#2}\label{subsec:exp2}
% Experiment structure
%
% Intro:
%   - background
%   - goal / experiment intention / why
%   - data
%   - evaluation
%   - reference to result table
%
% Procedure (by data if more than one):
%   - pre-processing
%   - feature extraction
%   - mapping
%   - feature representation
%   - classifier
%
% Remarks (if any)
%
% Result highlights:
%   - (only a description)

%% Experiment intro
In order to determine the optimal size of the codebook when using \bow, this experiment evaluates several codebook sizes on SERI dataset.

%% Experiment procedure
For this experiment, several pre-processing strategies are evaluated: (i) \nlm, (ii) a combination of \nlm and flattening; (iii) a combination of \nlm, flattening and aligning.
\lbp and \lbptop descriptors are detected using the default configuration.
Volumes are represented using the high-level feature extraction by using \bow, where the codebook size has been varied as $k \in \{10, 20, 30, \cdots, 100, 200, \cdots, 500, 1000\}$.
Finally, the volumes are classified using \lr. The choice of a linear classifier avoids that the results get busted by the classifier. In this manner any improvement would be linked to the pre-processing and the size of the codebook.
\input{content/experiment/Table2.tex}
\input{content/experiment/Table2-2.tex}


The usual construction of the codebook consists of clustering the samples in
the feature space using $k$-means. However, this operation is rather
computationally expensive and convergence of the $k$-means algorithm for all
codebook sizes is not granted.
Nonetheless, Nowak\,\textit{et al.}~\cite{nowak2006sampling} pointed out that randomly generated codebooks can be used at the expenses of accuracy.
Since the goal is to assess the best codebook size not its final performance, for this experiment, the construction of the codebook has been carried out using random initialization $k$-means++ algorithm ~\cite{arthur2007k}, which is usually used as a $k$-means initialization algorithm.

Figure.~\ref{fig:RBOW} shows the \ac{acc} and \ac{f1} score graphs obtained for a single case \footnote{Full set of scores can be found at the github repository} in~\cite{Lemaitre2015}, while the optimal number of words for all the configuration are reported in a compact manner in Table~\ref{tab:table2}.

\added[id=sik]{For the sake of comparison, Table~\ref{tab:table2-2} reports performance of optimal codebook size in terms of \ac{se} and \ac{sp}.}

%% Experiment Result description
In general, the obtained results show, that commonly less number of words is required when higher number of sampling points and radius ($\{S,R\} = \{24,3\}$) is used.
The required number of words decreases for \emph{local}-\ac{lbp} in comparison to \emph{global}-\ac{lbp} as well.
Although it was anticipated that the use of different pre-processing steps affect the optimal number of words, this influence is not substantial and consistent over all the obtained results.


\subsection{Experiment \#3}\label{subsec:exp3}
% Experiment structure
%
% Intro:
%   - background
%   - goal / experiment intention / why
%   - data
%   - evaluation
%   - reference to result table
%
% Procedure (by data if more than one):
%   - pre-processing
%   - feature extraction
%   - mapping
%   - feature representation
%   - classifier
%
% Remarks (if any)
%
% Result highlights:
%   - (only a description)

%% Experiment intro
Once studied the impact of the codebook size in Sect.\,\ref{subsec:exp2}, this experiment explores the improvement associated to use more sophisticated classification strategies.

%% Procedure
For this experiment, several pre-processing strategies are evaluated: (i) \nlm, (ii) a combination of \nlm and flattening; (iii) a combination of \nlm, flattening and aligning.
\lbp and \lbptop features are detected using the default configuration.
Volumes are represented using the high-level feature extraction, \bow. The codebooks are computed using regular $k$-means algorithm which is initialized by $k$-means++, where $k$ is chose accordingly to the findings in \emph{Experiment \#2}.
Finally, the volumes are classified using $k$-\ac{nn}, \ac{rf}, \ac{gb}, and \ac{svm}.

Regarding the classification strategies, $k$-\ac{nn} classifier is trained by considering the 3 nearest neighbor.
The \ac{rf} and \ac{gb} classifier are trained using 100 un-pruned trees, while \ac{svm} classifier is trained with \ac{rbf} kernel.

Table~\ref{tab:table3} shows the obtained results from this experiment, where the best performance are highlighted in \textbf{bold}.

%% Experiment Result description

In general terms, it is observed that \ac{rbf}-\ac{svm} provides the best results and outperforms the others.
This classifier achieve the highest \ac{se} and \ac{sp} of 81.2\% and 93.7\% , respectively using the high-level extraction of \emph{local}-\ac{lbp} descriptor, when the data was preprocessed using \ac{nlm} and \ac{f}.
In general, the classifier achieves the best performance,using the high-level extracted features of \emph{local}-\ac{lbp} and \emph{local}-\ac{lbptop}.
This results are obtained while using a sampling points and radius of $\{S,R\} = \{8,1\}$.

Although the highest results was achieved after additional \ac{f} pre-processing step, the effect of this step in not substantial and consistent through the whole experiment with respect to different classifiers and sampling points.
Moreover, the results indicate that often using the pre-processed data only with \ac{nlm} is sufficient enough.




\subsection{Experiment \#4}\label{subsec:exp4}
% Experiment structure
%
% Intro:
%   - background
%   - goal / experiment intention / why
%   - data
%   - evaluation
%   - reference to result table
%
% Procedure (by data if more than one):
%   - pre-processing
%   - feature extraction
%   - mapping
%   - feature representation
%   - classifier
%
% Remarks (if any)
%
% Result highlights:
%   - (only a description)

%% Experiment intro
This experiment replicates the \emph{Experiment \#3} for the case of low-level extracted features from the volumes.

%% Experiment procedure
For this experiment, several pre-processing strategies are evaluated: (i) \nlm, (ii) a combination of \nlm and flattening; (iii) a combination of \nlm, flattening and aligning.
\lbp and \lbptop descriptors are detected using the default configuration.
Volumes are represented using low-level feature extraction of the \emph{global} mapping.
Finally, the volumes are classified using $k$-\ac{nn}, \ac{rf}, \ac{gb}, and \ac{svm}; using the same configuration of \emph{Experiment \#3}.

The obtained results from this experiment is listed in Table.~\ref{tab:table4}, where the highest results are highlighted in \textbf{bold}.


%% Experiment Result description
The obtained results, shows that \ac{rf} has a better performance while using low-level extracted features, in comparison to the previous experiment where, \ac{svm} had a better performance dealing with high-level extracted features.

The highest results of this experiment, \ac{se} and \ac{sp} of 81.2\% and 81.2\%, respectively, was achieved with \ac{rf} and using \emph{global}-\ac{lbptop} features with sampling points and radius of $\{S,R\}=\{24,3\}$.
In general, in this experiment, \emph{global}-\ac{lbptop} features have better performance in comparison to \emph{global}-\ac{lbp} features and the  classification rates improved while using a higher number of sampling points and radius ($\{S,R\}=\{24,3\}$).

Similar to the previous experiments, although the effects of additional pre-processing steps (\ac{f} and \ac{fal}) is evident for \ac{rf} performance on $\{S,R\} = \{24,3\}$, similar to the previous experiments, this influence is not consistent for all different configurations, in terms of classifier and $\{S,R\}$.


\input{content/experiment/Table3.tex}
\input{content/experiment/Table4.tex}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../main.tex"
%%% End:
